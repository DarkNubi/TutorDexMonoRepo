Yes — having full historical messages will help a lot, but you want to select examples in a way that (a) improves robustness and (b) doesn’t “teach” the model obsolete templates or rare boilerplate that it will start hallucinating.

What historical data is good for (in prompting)
Coverage of real variants: different templates, field orderings, shorthand, missing fields, online vs physical, multi-slot, etc.
Reducing hallucination: by including “negative” examples where certain boilerplate is absent, and the correct JSON leaves those fields null/empty.
Agency-specific quirks: e.g., Mindflex often has “To apply:” lines, WhatsApp numbers, etc. You can teach the model to ignore those.
What not to do
Don’t just pick “most different ever” across all history. That tends to surface:
very old templates
one-off weird posts
spam/forwarded/compilation styles
…and then the model overgeneralizes.
Recommended approach: “recent-first + diversity + constraint balance”
1) Time-windowed sampling (avoid old templates)
For each agency, select examples from recent windows, e.g.:

last 7 days (highest weight)
last 30 days (medium weight)
last 90 days (low weight)
Only pull older than 90d if you know the agency reuses those formats.
This solves the “template drift” issue.

2) Diversity selection within each window
Within a given window, choose examples by cluster diversity, not raw “most different”.
Two practical options:

Option A (cheap, no embeddings): TF‑IDF + clustering

Vectorize normalized text (strip URLs/phones/emojis)
K-means into K clusters (e.g., 10–30 depending on volume)
Pick 1–2 representative posts per cluster (closest to centroid)
This tends to pick “typical variants” instead of outliers.
**Option B (better): embeddings + clustering

Embed each message (after normalization)
Cluster (HDBSCAN or k-means)
Pick centroid-near samples per cluster
Add a few “boundary” samples per cluster (but not the extreme farthest)
Key point: centroid samples > outliers for few-shot reliability.

3) Add explicit “hallucination guards” via counterexamples
For fields that are causing problems (commission / commitment / A-level):

For each agency, include:
examples where the phrase exists and is correctly captured in additional_remarks
examples where it does not exist, and additional_remarks does not mention it
This trains the model “only include it when present”.
4) Keep prompts fresh (rotation)
Instead of a static file forever:

Maintain an “example pool” per agency
Periodically regenerate the few-shot set from the last N days + diversity strategy
Keep a small “golden set” of edge cases that you know are important
5) Separate “format variability” from “content requirements”
If Mindflex templates change, you want examples that show:

new layouts
different field ordering
…but not necessarily unusual requirement clauses that are rare (those cause your current hallucinations).
So for “different era templates”: include them only if they still appear or if they represent a recurring pattern.

Concrete recommendation for TutorDex
Per agency build:
8–15 examples total in the prompt file
drawn from last 30 days, clustered into ~8 clusters, pick 1 per cluster
add 2–3 “negative” examples specifically designed to suppress hallucinations (no commission, no commitment, no A-level).
Update your system prompt with a strict line:
“additional_remarks must only contain clauses explicitly present in the message. Do not add typical agency requirements unless quoted.”
If you want the “best” long-term solution
Move away from heavy few-shot reliance:

Use fewer examples + stronger rules + post-processing validation
Use your new edit/delete monitor + historical scrape to build a small labeled dataset and fine-tune later (optional)
If you confirm what “historical scrape” format you’ll have (JSONL with raw_text, channel, timestamp?), I can outline an exact script pipeline to generate agency example files automatically (including “anti-hallucination” counterexamples) and keep them updated.