# TutorDexAggregator/.env.example
# Copy to `TutorDexAggregator/.env` and fill values. Do NOT commit secrets.

# ----------------------------
# Telegram reader (required)
# ----------------------------
TELEGRAM_API_ID=
TELEGRAM_API_HASH=
CHANNEL_LIST=["t.me/TuitionAssignmentsSG","t.me/tuitionassignmentsttrsg","t.me/TutorAnywhr","t.me/elitetutorsg","t.me/FTassignments"]

# Optional reader tuning
# HISTORIC_FETCH=0
# PROCESSED_STORE=processed_ids.json
# TELEGRAM_MAX_RETRIES=5
# TELEGRAM_INITIAL_RETRY_DELAY=1.0
# TELEGRAM_MAX_RETRY_DELAY=300.0
# TELEGRAM_BACKOFF_MULTIPLIER=2.0

# ----------------------------
# Telegram session (choose one)
# ----------------------------
# SESSION_STRING=
# TG_SESSION=tutordex.session

# ----------------------------
# LLM (required for extraction)
# ----------------------------
LLM_API_URL=http://host.docker.internal:1234
# LLM_MODEL_NAME=nuextract-v1.5@q6_k_l
# LLM_TIMEOUT_SECONDS=200
# Optional: system prompt override without editing `extract_key_info.py`
# LLM_SYSTEM_PROMPT_FILE=prompts/system_prompt_live.txt
# LLM_SYSTEM_PROMPT_TEXT="..."
# Optional: A/B examples without editing code
# LLM_INCLUDE_EXAMPLES=true
# LLM_EXAMPLES_VARIANT=default
# LLM_EXAMPLES_DIR=message_examples

# ----------------------------
# llama.cpp server (optional)
# ----------------------------
# If set, you can run `start_llama_server_loop.bat` (Windows) to start llama-server for you.
# LLAMA_SERVER_EXE=C:\\llama-bin\\llama-server.exe
# LLAMA_MODEL_PATH=C:\\models\\LFM2-8B-A1B-Q4_K_M.gguf
# LLAMA_SERVER_HOST=127.0.0.1
# LLAMA_SERVER_PORT=1234
# LLAMA_CTX=8192
# LLAMA_THREADS=6
# LLAMA_BATCH=512
# LLAMA_NGL=999
# LLAMA_SERVER_ARGS=

# Optional: Nominatim User-Agent (recommended if you do heavy backfills)
# NOMINATIM_USER_AGENT=TutorDexAggregator/1.0 (your-email@example.com)
#
# Optional: Nominatim reliability tuning (defaults are safe; increase only if needed)
# NOMINATIM_RETRIES=3
# NOMINATIM_BACKOFF_SECONDS=1.0

# Optional enrichment toggles
# DISABLE_NOMINATIM=false
# LOG_ASSIGNMENT_JSON=false

# Geo enrichment (offline datasets)
# GEO_ENRICHMENT_ENABLED=1
#
# Defaults:
# REGION_GEOJSON_PATH=TutorDexAggregator/data/2019_region_boundary.geojson
# MRT_DATA_JSON_PATH=TutorDexAggregator/data/mrt_data.json

# ----------------------------
# Bot 1: Broadcast to channel (optional)
# ----------------------------
# GROUP_BOT_TOKEN=
# 
# Single channel (backward compatible):
# AGGREGATOR_CHANNEL_ID=-1001234567890
#
# Multiple channels (JSON array, recommended):
# AGGREGATOR_CHANNEL_IDS=["-1001234567890", "-1009876543210"]
#
# BOT_API_URL=
# BROADCAST_FALLBACK_FILE=outgoing_broadcasts.jsonl
# BROADCAST_MAX_REMARKS_LEN=700
# BROADCAST_MAX_MESSAGE_LEN=3900
# BROADCAST_MAX_ATTEMPTS=4
# BROADCAST_RETRY_BASE_SECONDS=2.0
# BROADCAST_RETRY_MAX_SLEEP_SECONDS=60.0
#
# Enable message tracking for channel sync/reconciliation (default: on)
# ENABLE_BROADCAST_TRACKING=1

# ----------------------------
# Skipped/moderation forwarding (optional)
# ----------------------------
# SKIPPED_MESSAGES_CHAT_ID=
# SKIPPED_MESSAGES_THREAD_ID=

# ----------------------------
# Bot 2: DM sending + matching backend (optional)
# ----------------------------
# DM_ENABLED=false
# DM_BOT_TOKEN=
# DM_BOT_API_URL=
# TUTOR_MATCH_URL=http://127.0.0.1:8000/match/payload
# BACKEND_API_KEY=
# DM_MAX_RECIPIENTS=50
# DM_FALLBACK_FILE=outgoing_dm.jsonl

# ----------------------------
# Supabase persistence (optional)
# ----------------------------
# Enable with SUPABASE_ENABLED=true
# SUPABASE_ENABLED=false
# Supabase URL routing (recommended if you run both Docker + Windows Python):
# - Docker containers can reach Kong via the Docker network hostname `supabase-kong`.
# - Host Python (Windows/macOS/Linux) cannot resolve `supabase-kong`; use a host-reachable URL/port.
#
# SUPABASE_URL_DOCKER=http://supabase-kong:8000             # Supabase self-host (Docker network; no /rest/v1 suffix)
# SUPABASE_URL_HOST=http://127.0.0.1:8001                   # Supabase self-host (host port; check `docker port supabase-kong`)
#
# Fallback (single value):
# SUPABASE_URL=https://<project-ref>.supabase.co            # Supabase Cloud
# SUPABASE_URL=https://supabase-api.example.com             # Supabase self-host (public URL; no /rest/v1 suffix)
# SUPABASE_SERVICE_ROLE_KEY=
# SUPABASE_ASSIGNMENTS_TABLE=assignments
# SUPABASE_BUMP_MIN_SECONDS=21600

# ----------------------------
# Raw Telegram persistence (optional, recommended for backfill)
# ----------------------------
# Persists lossless message history for reprocessing/dedupe/analytics.
# Defaults to SUPABASE_ENABLED if SUPABASE_RAW_ENABLED is not set.
# SUPABASE_RAW_ENABLED=true
# SUPABASE_RAW_CHANNELS_TABLE=telegram_channels
# SUPABASE_RAW_MESSAGES_TABLE=telegram_messages_raw
# SUPABASE_RAW_RUNS_TABLE=ingestion_runs
# SUPABASE_RAW_PROGRESS_TABLE=ingestion_run_progress
#
# Optional local fallback (writes JSONL when Supabase raw is disabled/unavailable):
# RAW_FALLBACK_FILE=raw_messages_fallback.jsonl

# ----------------------------
# Extraction queue (collector -> telegram_extractions)
# ----------------------------
# Collector defaults to enqueuing when this is unset. Set to 0/false to disable.
# EXTRACTION_QUEUE_ENABLED=true
# EXTRACTION_PIPELINE_VERSION=2026-01-02_det_time_v1
#
# Extraction worker behavior (defaults are on; Mode 4 runs these as 0 + oneshot)
# EXTRACTION_WORKER_BROADCAST=1
# EXTRACTION_WORKER_DMS=1
# EXTRACTION_WORKER_ONESHOT=0
# EXTRACTION_WORKER_MAX_JOBS=0
#
# Hardened pipeline toggles (recommended for det_time_v1)
# USE_DETERMINISTIC_TIME=1
# HARD_VALIDATE_MODE=enforce
# ENABLE_DETERMINISTIC_SIGNALS=1
# USE_NORMALIZED_TEXT_FOR_LLM=0

# ----------------------------
# Subject taxonomy (optional, recommended)
# ----------------------------
# Adds derived arrays for hierarchical filtering + analytics:
# - subjects_general (broad categories like MATH)
# - subjects_canonical (advanced codes like MATH.SEC_EMATH)
#
# SUBJECT_TAXONOMY_ENABLED=false
# SUBJECT_TAXONOMY_DEBUG=false  # Only enable if you also add a `canonicalization_debug` jsonb column.
# SUBJECT_TAXONOMY_PATH=taxonomy/subjects_taxonomy_v1.json

# ----------------------------
# Logging (optional)
# ----------------------------
# LOG_LEVEL=INFO
# LOG_JSON=false
# LOG_DIR=logs
# LOG_TO_CONSOLE=true
# LOG_TO_FILE=true
# LOG_FILE=tutordex_aggregator.log
# LOG_MAX_BYTES=5000000
# LOG_BACKUP_COUNT=5

# ----------------------------
# Observability (Alertmanager -> Telegram)
# ----------------------------
# Used by `observability/alertmanager/alertmanager.yml` (with `--config.expand-env`).
#
# Telegram alert destination (recommend: a private admin group + a dedicated thread)
# ALERT_BOT_TOKEN=            # can reuse GROUP_BOT_TOKEN, but better to use a separate ops bot token
# ALERT_CHAT_ID=              # numeric chat id (e.g. -100...)
# ALERT_THREAD_ID=            # optional topic/thread id inside the chat
# ALERT_PREFIX=[TutorDex]

# OpenTelemetry (optional; Tempo is wired via docker compose)
# OTEL_ENABLED=true
# OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318

# ----------------------------
# TutorCity API fetch (no LLM)
# ----------------------------
# TUTORCITY_API_URL=https://tutorcity.sg/api/tuition-assignments/
# TUTORCITY_LIMIT=50
# TUTORCITY_TIMEOUT_SECONDS=30
# TUTORCITY_FETCH_INTERVAL_SECONDS=300
# TUTORCITY_USER_AGENT=TutorDexTutorCityFetcher/1.0
#
# Backend health aggregation (checks backend + redis + supabase via /health/full)
# MONITOR_BACKEND_HEALTH_URL=http://127.0.0.1:8000/health/full

# ----------------------------
# Telegram edit/delete monitor (optional)
# ----------------------------
# Records message edits and deletions for CHANNEL_LIST into a local SQLite DB.
# Useful because some agencies edit posts after publishing.
#
# Run:
# - `python monitor_message_edits.py`
#
# EDIT_MONITOR_DB_PATH=monitoring/telegram_message_edits.sqlite
# EDIT_MONITOR_EVENTS_JSONL=monitoring/telegram_message_edits_events.jsonl
# EDIT_MONITOR_INCLUDE_TEXT=true
# EDIT_MONITOR_MAX_TEXT_CHARS=20000
# EDIT_MONITOR_HISTORIC_FETCH=50
# EDIT_MONITOR_SUMMARY_INTERVAL_SECONDS=600

# ----------------------------
# Freshness tier updater (optional)
# ----------------------------
# Example scheduled runs:
# - `python update_freshness_tiers.py --dry-run`
# - `python update_freshness_tiers.py --expire-action none`
#
# FRESHNESS_TIER_ENABLED=false  # set true after applying the migration
# FRESHNESS_TIERS_INTERVAL_SECONDS=3600  # docker sidecar cadence (default: hourly)
# FRESHNESS_TIERS_ARGS="--expire-action closed --red-hours 336"  # passed to update_freshness_tiers.py

# ----------------------------
# Extraction worker tuning (queue pipeline)
# ----------------------------
# EXTRACTION_PIPELINE_VERSION=singlecall_v2
# EXTRACTION_WORKER_BATCH=10
# EXTRACTION_WORKER_IDLE_S=2
# EXTRACTION_WORKER_BROADCAST=0   # recommended for backfills
# EXTRACTION_WORKER_DMS=0         # recommended for backfills
# EXTRACTION_MAX_ATTEMPTS=3
# EXTRACTION_BACKOFF_BASE_S=1.0
# EXTRACTION_BACKOFF_MAX_S=20.0
# EXTRACTION_STALE_PROCESSING_SECONDS=900
#
# Auto-sync broadcast channels on worker startup (reconciles channel with DB)
# BROADCAST_SYNC_ON_STARTUP=0

# ----------------------------
# Compilation detection tuning (used by the extraction worker)
# ----------------------------
# When a message looks like a "compilation" (multi-assignment), extraction is skipped.
# COMPILATION_CODE_HITS=2
# COMPILATION_LABEL_HITS=2
# COMPILATION_POSTAL_HITS=2
# COMPILATION_URL_HITS=2
# COMPILATION_BLOCK_COUNT=3

# ----------------------------
# Legacy ingester (read_assignments.py) tuning (only if you run `python runner.py start`)
# ----------------------------
# read_assignments.py has its own compilation heuristics and optional bumping rules.
# COMPILATION_FILE=compilations.jsonl
# COMPILATION_BUMP_ENABLED=true
# FORWARDED_BUMP_ENABLED=true
#
# Legacy compilation thresholds (read_assignments.py only)
# COMP_CODE_HITS=3
# COMP_LABEL_HITS=5
# COMP_POSTAL_HITS=3
# COMP_URL_HITS=3
# COMP_BLOCK_COUNT=12
